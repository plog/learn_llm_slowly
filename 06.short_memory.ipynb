{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Short-Term Memory\n",
    "\n",
    "Must read:\n",
    "- [What is Memory?](https://langchain-ai.github.io/langgraph/concepts/memory/#what-is-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model with specific settings\n",
    "llm = ChatOllama(\n",
    "    base_url=os.getenv(\"OLLAMA_SERVER\"),\n",
    "    model = \"llama3.2:latest\",\n",
    "    temperature = 0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a verys simple short-term memory to store conversation history\n",
    "class ShortTermMemory:\n",
    "    def __init__(self, max_tokens=1024):\n",
    "        self.history = []  # To store conversation history\n",
    "        self.max_tokens = max_tokens  # Maximum allowed token count\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_trimmed_history(self):\n",
    "        total_tokens = sum(len(msg[\"content\"].split()) for msg in self.history)\n",
    "        while total_tokens > self.max_tokens and self.history:\n",
    "            total_tokens -= len(self.history.pop(0)[\"content\"].split())\n",
    "        return self.history\n",
    "\n",
    "    def reset(self):\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize short-term memory\n",
    "memory = ShortTermMemory(max_tokens=1024)\n",
    "# Reset the memory\n",
    "memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat function\n",
    "# it will take user input and return the AI response\n",
    "# then it will store the conversation history in the memory\n",
    "def chat(user_input):\n",
    "    system_message = {\"role\": \"system\", \"content\": \"Please respond to all messages in French.\"}\n",
    "    memory.add_message(\"user\", user_input)\n",
    "    prompt = [system_message] + memory.get_trimmed_history()\n",
    "    response = llm.invoke(prompt)\n",
    "    ai_reply = response.content\n",
    "    ai_reply = \"\\n\".join(line for line in ai_reply.splitlines() if line.strip())\n",
    "    memory.add_message(\"assistant\", ai_reply)    \n",
    "    return ai_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some system initialization messages\n",
    "memory.add_message(\"system\", \"Always answer in French with short and precise sentences.\")\n",
    "# A list of questions to challenge them memory\n",
    "test_inputs = [\n",
    "    \"My name is Alex?\",\n",
    "    \"What's the capital of France?\",\n",
    "    \"Who won the World Cup in 2018?\",\n",
    "    \"Do you remember my name?\",\n",
    "    \"Can you tell me what we talked about earlier?\",\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"Give me all questions I asked you so far\",\n",
    "    \"What did we discuss about my name?\",\n",
    "    \"What is 2+2?\",\n",
    "    \"What did I ask you about the World Cup?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the conversation\n",
    "# You can put this in a While True loop to keep the conversation going\n",
    "# But it's not nice in a Jupyter notebook\n",
    "for i, user_input in enumerate(test_inputs, start=1):\n",
    "    print(\"-\" * 100)\n",
    "    print(Fore.GREEN +f\"You: {i}. {user_input}\")\n",
    "    reply = chat(user_input)\n",
    "    print(Fore.YELLOW +f\"IA: {reply}\")\n",
    "print(Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's in the memory?\n",
    "mem = memory.get_trimmed_history()\n",
    "for m in mem:\n",
    "    print(m[\"role\"], m[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
