{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tools\n",
    "\n",
    "Must read:\n",
    "- [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/)\n",
    "- [How to pass tool outputs to chat models](https://python.langchain.com/docs/how_to/tool_results_pass_to_model/)\n",
    "\n",
    "Tool calling, also known as function calling, enables AI models to interact with systems like APIs or databases by responding in a schema-specific format. It involves four key steps: tool creation (using the @tool decorator to define a tool with its schema), tool binding (connecting the tool to a model via .bind_tools()), tool calling (where the model determines when to use the tool based on input relevance), and tool execution (where the tool runs using model-provided arguments). \n",
    "\n",
    "<img src=\"https://python.langchain.com/assets/images/tool_calling_concept-552a73031228ff9144c7d59f26dedbbf.png\" height=\"200\">\n",
    "\n",
    "LangChain provides a standardized interface for integrating tools with models, supporting workflows where the model invokes tools only when necessary, ensuring responses conform to the tool’s schema. Best practices include using simple, well-named, narrowly scoped tools to enhance model performance and usability.\n",
    "\n",
    "1) **Tool Creation**: Use the @tool decorator to create a tool. A tool is an association between a function and its schema. \n",
    "2) **Tool Binding**: The tool needs to be connected to a model that supports tool calling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model with specific settings\n",
    "llm = ChatOllama(\n",
    "    base_url=os.getenv(\"OLLAMA_SERVER\"),\n",
    "    model = \"mistral\",\n",
    "    temperature = 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without tools you would get a simple LLM answer\n",
    "result = llm.invoke(\"who are you?\")\n",
    "print(f\"Simple LLM result: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if you want to use the tool, you need to \"bind\" it to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools=[multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, you can use the tool in your queries\n",
    "query    = \"What is 2 multiplied by 3?\"\n",
    "messages = [HumanMessage(content=query)]\n",
    "result   = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method one: execute the tool call directly\n",
    "# The result will contain the tool call\n",
    "for tool_call in getattr(result, \"tool_calls\", []):\n",
    "    if tool_call[\"name\"] == \"multiply\":\n",
    "        args = tool_call[\"args\"]\n",
    "        tool_result = multiply.run(args)  # Pass arguments as a dictionary\n",
    "        print(f\"Tool Execution Result: {args} => {tool_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method two: execute the tool call using the LLM\n",
    "# Using .invoke() is a valid and powerful approach when handling tools, \n",
    "# as it ensures that the tool execution is consistent with LangChain’s \n",
    "# interface and allows for additional flexibility (e.g., returning AI messages). \n",
    "\n",
    "messages = []  # This will store the sequence of interactions\n",
    "# Process each tool call in the response\n",
    "for tool_call in getattr(result, \"tool_calls\", []):\n",
    "    # Map the tool name to the corresponding tool function\n",
    "    selected_tool = {\"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    # Execute the tool using `.invoke()` and add the response to messages\n",
    "    tool_msg = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "# Print the updated messages list\n",
    "for msg in messages:\n",
    "    print(f\"Tool Response: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is output format and intended use:\n",
    "\n",
    "1.\t`.invoke()`: Returns a structured result (like AIMessage) aligned with LangChain’s schema, making it suitable for workflows requiring conversational context or chaining multiple tools.\n",
    "2.\t`.run()`: Directly executes the tool and returns a raw result (e.g., 6), which is simpler and better for standalone use.\n",
    "\n",
    "Use `.invoke()` for structured workflows; use `.run()` for quick, direct execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
