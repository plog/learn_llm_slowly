{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f00af6",
   "metadata": {},
   "source": [
    "# 2. Vectors and embedding\n",
    "\n",
    "The notebook is a step-by-step guide to processing documents and creating vector representations (embeddings) of their content. Here's a simplified explanation of what it does:\n",
    "\n",
    "1) **Load the Document**: It starts by loading a document (e.g., a PDF file) into the program.\n",
    "2) **Split the Document**: The document is then split into smaller chunks to make it easier to process.\n",
    "3) **Create Embeddings**: For each chunk, the notebook uses a model to create a vector representation (embedding). This is like converting the text into a series of numbers that capture its meaning.\n",
    "4) **Store the Embeddings**: These embeddings are stored in a special database called a vector store. This allows for efficient searching and retrieval based on the content of the documents.\n",
    "5) **Query the Vector Store**: Finally, the notebook demonstrates how to query this vector store to find relevant document chunks based on a search query.\n",
    "\n",
    "Overall, the notebook shows how to transform text documents into a format that can be easily searched and analyzed using machine learning techniques.\n",
    "\n",
    "# Qdrant\n",
    "Qdrant is a high-performance vector database used to store and search vector representations (embeddings) of data efficiently. It is scalable and integrates well with machine learning and AI applications, making it suitable for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5969f5c-d7f2-48e1-8f4a-d28fa07a2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4221d1-063d-4ed0-bd16-8130a56e96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_SERVER = os.getenv(\"OLLAMA_SERVER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9c0a4-7bb8-49af-8fe4-a16eb4d3174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"./docs/test.pdf\"\n",
    "# Load a PDF document and split it into individual pages as document objects\n",
    "loader    = PyPDFLoader(file_path)\n",
    "docs      = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf92d3c-9568-4a59-92dd-96590af56b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the first document\n",
    "print(f\"{docs[0].page_content[:100]}\\n\")\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227afb8-8bd0-4db1-bd35-f8bc69e94ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=10, \n",
    "    add_start_index=True\n",
    ")\n",
    "# Split the documents into chunks\n",
    "all_splits    = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8277d903-5525-4b93-9f42-0bbadd738425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=OLLAMA_SERVER, \n",
    "    model=\"mxbai-embed-large\"\n",
    ")\n",
    "vectors    = [embeddings.embed_query(text.page_content) for text in all_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2051dd1-b106-45dd-8c75-c0866ab030bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the QDrant client and recreate a collection\n",
    "# For this tutorial, we delete the collection if it already exists\n",
    "# This is to ensure that the collection is created from scratch\n",
    "qdrant_client     = QdrantClient(host='lawboxai_qdrant')\n",
    "qdrant_collection = 'example'\n",
    "qdrant_client.delete_collection(collection_name=qdrant_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded3383-5814-4d5e-b183-6a03ff36a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new collection with the same name\n",
    "qdrant_client.create_collection(\n",
    "   collection_name=qdrant_collection,\n",
    "   vectors_config=models.VectorParams(\n",
    "       size=len(vectors[0]), \n",
    "       distance=models.Distance.COSINE\n",
    "    ),\n",
    ")\n",
    "# Add the documents to the collection\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=qdrant_collection,\n",
    "    embedding=embeddings\n",
    ")\n",
    "vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88664e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is not a LLM model query, but a similarity search in the vector\n",
    "# We will do a more advanced query with LLM in the next tutorial\n",
    "query = \"I'm looking for Margaret\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382fbb8-7fb9-44b4-8c4b-d2d1776a922d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similarity_search: Returns the top documents based solely on vector similarity\n",
    "docs  = vector_store.similarity_search(query, k=10,)\n",
    "for doc in docs:\n",
    "    print(50*'-')\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e592b4-14f6-4ec0-882a-882e209e3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to similarity_search, but also returns relevance scores\n",
    "docs  = await vector_store.asimilarity_search_with_relevance_scores(query,k=5)   #.similarity_search(query, k=10,)\n",
    "for doc in docs:\n",
    "    print(50*'-',doc[1])\n",
    "    print(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9310b1-c856-40fc-8ce7-652f2a88a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LangChain way, using Retriever\n",
    "# Retrievers are a higher-level abstraction that wraps the VectorStore\n",
    "# They provide a more user-friendly interface to the VectorStore\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieved_docs = retriever.invoke(query,k=7)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "for doc in retrieved_docs:\n",
    "    print(50*'-')\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a883b-d74d-4700-a243-dba4c98333d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
