{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Long-Term memory\n",
    "\n",
    "Must read:\n",
    "- [What is Memory?](https://langchain-ai.github.io/langgraph/concepts/memory/#what-is-memory)\n",
    "\n",
    "This code creates a chatbot that can remember things you tell it and use those memories to give better responses. \n",
    "\n",
    "1) Stores what you say in Qdrant after turning your words into vectors so the bot can find related information later.\n",
    "2) When you ask the bot something, it checks the database for anything related to your question. \n",
    "3) It uses this information, along with your current input, to create a reply that feels like it “remembers” past conversations. \n",
    "4) If you tell it to “remember” something, it saves both your words and its reply, so it can recall them in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "import uuid\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.models import PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "import os\n",
    "OLLAMA_SERVER = os.getenv(\"OLLAMA_SERVER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatOllama and Qdrant client\n",
    "llm               = ChatOllama(base_url=OLLAMA_SERVER, model=\"mistral\", temperature=0.8)\n",
    "embeddings        = OllamaEmbeddings(base_url=OLLAMA_SERVER, model=\"mxbai-embed-large\")\n",
    "qdrant_client     = QdrantClient(host='lawboxai_qdrant')\n",
    "qdrant_collection = \"long_term_memory\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongTermMemoryManager:\n",
    "    def __init__(self, collection_name, qdrant_client, embeddings):\n",
    "        self.collection_name = collection_name\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # Create collection if it doesn't exist\n",
    "        if collection_name not in [c.name for c in qdrant_client.get_collections().collections]:\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=len(self.embeddings.embed_query(\"test\")),distance=\"Cosine\"))\n",
    "\n",
    "    def store_memory(self, content):\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=[PointStruct(\n",
    "                id=str(uuid.uuid5(uuid.NAMESPACE_DNS, content)),\n",
    "                vector=self.embeddings.embed_query(content),\n",
    "                payload={\"content\": content}\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    def retrieve_memory(self, query, top_k=3):\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=self.embeddings.embed_query(query),\n",
    "            limit=top_k\n",
    "        )\n",
    "        return [point.payload[\"content\"] for point in results.points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"long_term_memory\"\n",
    "memory_manager = LongTermMemoryManager(collection_name, qdrant_client, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_memory(user_input):\n",
    "    # Retrieve relevant memories\n",
    "    relevant_memories = memory_manager.retrieve_memory(user_input)\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Please respond in French, and incorporate any relevant context from the following: \" + \" \".join(relevant_memories)\n",
    "    }\n",
    "    prompt = [\n",
    "        system_message,\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    response = llm.invoke(prompt)\n",
    "    ai_reply = response.content.strip()\n",
    "    if \"remember\" in user_input.lower():\n",
    "        memory_manager.store_memory(user_input)\n",
    "        memory_manager.store_memory(ai_reply)\n",
    "    \n",
    "    return ai_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = [\n",
    "    \"I told you something about Paris. Can you recall?\",\n",
    "    \"Do you remember what I told you about myself?\",\n",
    "    \"Tell me something about my name and where I live.\",\n",
    "    \"Hello, my name is Alex.\",\n",
    "    \"Can you remember my name?\",\n",
    "    \"Remember that I live in Paris.\",\n",
    "    \"Where do I live?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Can you summarize what we discussed earlier?\",\n",
    "    \"Goodbye, and remember my details for next time.\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(interactions, 1):\n",
    "    print(f\"Interaction {i}\")\n",
    "    print(f\"You: {user_input}\")\n",
    "    reply = chat_with_memory(user_input)\n",
    "    print(f\"Assistant: {reply}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
