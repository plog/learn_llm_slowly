{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Runnable\n",
    "\n",
    "Let's step back a little... A [fundamental concept](https://python.langchain.com/v0.2/docs/concepts/) of LangChain is the Runnable interface. This the \"chain\" in Lang...Chain\n",
    "\n",
    "The [Runnable](https://python.langchain.com/docs/concepts/runnables/) interface in LangChain provides a consistent and versatile way to interact with components like language models, retrievers, and tools, allowing them to be invoked, batched, streamed, and inspected. \n",
    "\n",
    "It enables efficient execution of tasks, supports asynchronous and parallel processing, and facilitates the composition of complex workflows through chaining.\n",
    "\n",
    "Runnables can also be customized with runtime configurations like concurrency limits, callbacks, and metadata, making them adaptable for diverse applications in AI and machine learning workflows.\n",
    "\n",
    "![alt text](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyVmJ3RFRjXS-czkbjx1MA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now I'll use a module to import our llm from tools.py\n",
    "from tools import llm # containing my ChatOllama\n",
    "from langchain.chains import TransformChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing (add context)\n",
    "def add_context(inputs):\n",
    "    user_input = inputs[\"input\"]\n",
    "    return {\"input\": f\"Context: You are a helpful assistant. {user_input}\"}\n",
    "\n",
    "# Step 2: Language model invocation (trivia generator)\n",
    "def generate_response(inputs):\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": inputs[\"input\"]}])\n",
    "    return {\"output\": response.content.replace(' ',' ~~ ')}\n",
    "\n",
    "# Step 3: Post-processing (make result uppercase)\n",
    "def clean_output(inputs):\n",
    "    output = inputs[\"output\"]\n",
    "    return {\"final_output\": output.upper()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define Runnable Chains that can be executed by the LangChain engine\n",
    "# Each chain is a sequence of transformations that are applied to the input data\n",
    "preprocess_chain = TransformChain(\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"input\"],\n",
    "    transform=add_context,\n",
    ")\n",
    "llm_chain = TransformChain(\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"output\"],\n",
    "    transform=generate_response,\n",
    ")\n",
    "postprocess_chain = TransformChain(\n",
    "    input_variables=[\"output\"],\n",
    "    output_variables=[\"final_output\"],\n",
    "    transform=clean_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pipeline that combines \"Runnable\" elements\n",
    "pipeline = preprocess_chain | llm_chain | postprocess_chain\n",
    "\n",
    "# Test the pipeline\n",
    "user_input = \"What is the capital of France?\"\n",
    "result = pipeline.invoke({\"input\": user_input})\n",
    "print(\"Final Output:\", result[\"final_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain, components process specific input types and produce corresponding outputs. \n",
    "\n",
    "Prompts create structured requests, ChatModels and LLMs handle text or messages, and OutputParsers refine results. \n",
    "\n",
    "Retrievers fetch documents based on a query, while Tools adapt their inputs and outputs based on their purpose, ensuring smooth and consistent workflows.\n",
    "\n",
    "| Runnable Component     | Input Type                                  | Output Type               |\n",
    "|---------------|---------------------------------------------|---------------------------|\n",
    "| Prompt        | dictionary                                  | PromptValue               |\n",
    "| ChatModel     | a string, list of chat messages, or a PromptValue | ChatMessage              |\n",
    "| LLM           | a string, list of chat messages, or a PromptValue | String                   |\n",
    "| OutputParser  | the output of an LLM or ChatModel           | Depends on the parser     |\n",
    "| Retriever     | a string                                    | List of Documents         |\n",
    "| Tool          | a string or dictionary, depending on the tool | Depends on the tool       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
