{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4a4dce",
   "metadata": {},
   "source": [
    "# 1. Talk to the LLM\n",
    "We will follow the [LangChain examples](https://python.langchain.com/docs/tutorials/llm_chain/) and adapt them a little to work with Ollama\n",
    " \n",
    "1) We set up the necessary libraries and connects to the Ollama server. It checks the LangChain version to ensure compatibility.\n",
    "2) Initializes the language model with specific settings.\n",
    "3) Creates a template that tells the model to respond in a certain language and includes a placeholder for user questions.\n",
    "4) Ask a question in a specific language and sends it to the model.\n",
    "5) Finally, it prints the model's response, showing how to get answers in the desired language.\n",
    "\n",
    "# We need\n",
    "\n",
    "- `langchain` The core LangChain library.\n",
    "- `ChatOllama`: The ChatOllama class for using Ollama models.\n",
    "- `ChatPromptTemplate`: ChatPromptTemplate for creating chat prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2de674c4-d55f-43ee-9ae4-dcbfb15a3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bb91f-0dcf-4f6f-92a9-a1f6c19c3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the version of LangChain to ensure compatibility\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4f2d11f-b3e2-466f-8b7d-846164b78c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model with specific settings\n",
    "llm = ChatOllama(\n",
    "    base_url=os.getenv(\"OLLAMA_SERVER\"),\n",
    "    model = \"llama3.2:latest\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a7545c2-280e-48fd-b341-3b7d6c756248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We Sets up the chat prompt template for a language model.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Always answer in {language}\"), \n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "832e33ff-096b-4dd4-a7ed-236bf375d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt by invoking the template with specific language and text\n",
    "prompt = prompt_template.invoke({\n",
    "    \"language\": \"French\", \n",
    "     \"text\": \"C'est o√π la Belgique?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2d309-b541-45ea-b85b-aa3045b984fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prompt to a list of messages\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a544961-1131-4ea3-9ff0-7e66f22601ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6b0f4-e0b7-4601-88bf-221101c53645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42fae9-80e2-476e-adc9-b861493c31b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
